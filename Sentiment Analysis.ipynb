{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "basic_sentiment_analysis\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "This module contains the code and examples described in \n",
    "http://fjavieralba.com/basic-sentiment-analysis-with-python.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker data read successfully!\n"
     ]
    }
   ],
   "source": [
    "class Splitter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        \n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "\n",
    "class DictionaryTagger(object):\n",
    "\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n",
    "\n",
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentence_score(sentence_tokens, previous_token, acum_score):    \n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentiment_score(review):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in review])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Read in the worker data\n",
    "    xls_file = pd.ExcelFile(\"Origami_Data.xlsx\", encoding = 'utf-16')\n",
    "    worker_data = xls_file.parse('Client Information')\n",
    "    print \"worker data read successfully!\"\n",
    "    neww = []\n",
    "    scores = []\n",
    "    comments = worker_data[\"Comments from the employer\"]    \n",
    "    #comments = pd.Series(comments).str.cat(sep=' ')\n",
    "\n",
    "    comments = comments.tolist()\n",
    "\n",
    "    for comment in comments:\n",
    "        if type(comment) is float:\n",
    "            neww.append(\"TTTT\")\n",
    "        if type(comment) is unicode:\n",
    "            neww.append(comment)\n",
    "\n",
    "    \n",
    "    for comment in neww:\n",
    "        if (comment == \"TTTT\"):\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            splitter = Splitter()\n",
    "            postagger = POSTagger()\n",
    "            dicttagger = DictionaryTagger(['dicts/positive.yml','dicts/negative.yml','dicts/inc.yml','dicts/dec.yml','dicts/inv.yml'])\n",
    "\n",
    "            splitted_sentences = splitter.split(\"Good worker. Willing to work extra hours.\")\n",
    "    #pprint(splitted_sentences)\n",
    "\n",
    "            pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    #pprint(pos_tagged_sentences)\n",
    "\n",
    "            dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    #pprint(dict_tagged_sentences)\n",
    "\n",
    "    #print(\"analyzing sentiment...\")\n",
    "    #score = sentiment_score(dict_tagged_sentences)\n",
    "            scores.append(sentiment_score(dict_tagged_sentences))\n",
    "\n",
    "    #splitter = Splitter()\n",
    "    #postagger = POSTagger()\n",
    "    #dicttagger = DictionaryTagger([ 'dicts/positive.yml', 'dicts/negative.yml', \n",
    "                                    \n",
    "\n",
    "    #splitted_sentences = splitter.split(comment)\n",
    "    #pprint(splitted_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker data read successfully!\n",
      "analyzing sentiment...\n",
      "-3.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "basic_sentiment_analysis\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "This module contains the code and examples described in \n",
    "http://fjavieralba.com/basic-sentiment-analysis-with-python.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "class Splitter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "\n",
    "class DictionaryTagger(object):\n",
    "\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n",
    "\n",
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentence_score(sentence_tokens, previous_token, acum_score):    \n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentiment_score(review):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in review])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    xls_file = pd.ExcelFile(\"Origami_Data.xlsx\", encoding = 'utf-16')\n",
    "    worker_data = xls_file.parse('Client Information')\n",
    "    print \"worker data read successfully!\"\n",
    "    comments = worker_data[\"Comments from the employer\"]    \n",
    "    comments = pd.Series(comments).str.cat(sep=' ')\n",
    "    \n",
    "    text = comments\n",
    "\n",
    "    splitter = Splitter()\n",
    "    postagger = POSTagger()\n",
    "    dicttagger = DictionaryTagger([ 'dicts/positive.yml', 'dicts/negative.yml', \n",
    "                                    'dicts/inc.yml', 'dicts/dec.yml', 'dicts/inv.yml'])\n",
    "\n",
    "    splitted_sentences = splitter.split(text)\n",
    "    #pprint(splitted_sentences)\n",
    "\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    #pprint(pos_tagged_sentences)\n",
    "\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    #pprint(dict_tagged_sentences)\n",
    "\n",
    "    print(\"analyzing sentiment...\")\n",
    "    score = sentiment_score(dict_tagged_sentences)\n",
    "    print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker data read successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "\n",
    "xls_file = pd.ExcelFile(\"Origami_Data.xlsx\", encoding = 'utf-16')\n",
    "worker_data = xls_file.parse('Client Information')\n",
    "print \"worker data read successfully!\"\n",
    "scores = []\n",
    "neww = []    \n",
    "comments = worker_data[\"Comments from the employer\"]    \n",
    "    #comments = pd.Series(comments).str.cat(sep=' ')\n",
    "\n",
    "comments = comments.tolist()\n",
    "\n",
    "for comment in comments:\n",
    "    if type(comment) is float:\n",
    "        neww.append(\"TTTT\")\n",
    "    if type(comment) is unicode:\n",
    "        neww.append(comment)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'G',\n",
       " 'o',\n",
       " 'o',\n",
       " 'd',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " 'e',\n",
       " 'r',\n",
       " '.',\n",
       " ' ',\n",
       " 'W',\n",
       " 'i',\n",
       " 'l',\n",
       " 'l',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " ' ',\n",
       " 'e',\n",
       " 'x',\n",
       " 't',\n",
       " 'r',\n",
       " 'a',\n",
       " ' ',\n",
       " 'h',\n",
       " 'o',\n",
       " 'u',\n",
       " 'r',\n",
       " 's',\n",
       " '.',\n",
       " '\\n',\n",
       " '1',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'S',\n",
       " 'e',\n",
       " 'e',\n",
       " 'm',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'k',\n",
       " 'n',\n",
       " 'o',\n",
       " 'w',\n",
       " 'l',\n",
       " 'e',\n",
       " 'd',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " 'u',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 'r',\n",
       " 'd',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'k',\n",
       " 'e',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '\\n',\n",
       " '2',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'L',\n",
       " 'i',\n",
       " 'k',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'c',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " ' ',\n",
       " 'b',\n",
       " 'u',\n",
       " 't',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'i',\n",
       " 'n',\n",
       " 'i',\n",
       " 't',\n",
       " 'e',\n",
       " 'l',\n",
       " 'y',\n",
       " ' ',\n",
       " 'n',\n",
       " 'o',\n",
       " 't',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'g',\n",
       " 'o',\n",
       " 'o',\n",
       " 'd',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 't',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '\\n',\n",
       " '3',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'W',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " ' ',\n",
       " '\\n',\n",
       " '4',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 'n',\n",
       " 'e',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 't',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 's',\n",
       " 'c',\n",
       " 'r',\n",
       " 'e',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'e',\n",
       " 'n',\n",
       " 'd',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'e',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '\\n',\n",
       " '5',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'L',\n",
       " 'a',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " '\\n',\n",
       " '6',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'O',\n",
       " 'n',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'u',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'g',\n",
       " 'o',\n",
       " 'o',\n",
       " 'd',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 't',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'm',\n",
       " ',',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'y',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '\\n',\n",
       " '7',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'T',\n",
       " 'h',\n",
       " 'a',\n",
       " 'n',\n",
       " 'k',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'n',\n",
       " 'd',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 's',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " 'o',\n",
       " 'n',\n",
       " 'e',\n",
       " '.',\n",
       " ' ',\n",
       " 'S',\n",
       " 'i',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'y',\n",
       " ' ',\n",
       " 'd',\n",
       " 'i',\n",
       " 'd',\n",
       " 'n',\n",
       " '\\xe2',\n",
       " '\\x80',\n",
       " '\\x99',\n",
       " 't',\n",
       " ' ',\n",
       " 'w',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '\\n',\n",
       " '8',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'W',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'l',\n",
       " 'o',\n",
       " 't',\n",
       " ',',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'm',\n",
       " 'o',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'g',\n",
       " 'o',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " ' ',\n",
       " 'g',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " 'n',\n",
       " '\\n',\n",
       " '9',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'E',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'o',\n",
       " 'y',\n",
       " 'e',\n",
       " 'e',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 't',\n",
       " 'i',\n",
       " 'r',\n",
       " 'e',\n",
       " 'd',\n",
       " '\\n',\n",
       " '1',\n",
       " '0',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'H',\n",
       " 'a',\n",
       " 'r',\n",
       " 'd',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'b',\n",
       " 'a',\n",
       " 'l',\n",
       " 'a',\n",
       " 'n',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'i',\n",
       " 'r',\n",
       " ' ',\n",
       " 's',\n",
       " 'c',\n",
       " 'h',\n",
       " 'e',\n",
       " 'd',\n",
       " 'u',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " ' ',\n",
       " 'h',\n",
       " 'e',\n",
       " 'a',\n",
       " 'l',\n",
       " 't',\n",
       " 'h',\n",
       " ' ',\n",
       " 'n',\n",
       " 'e',\n",
       " 'e',\n",
       " 'd',\n",
       " 's',\n",
       " '\\n',\n",
       " '1',\n",
       " '1',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'F',\n",
       " 'a',\n",
       " 'n',\n",
       " 't',\n",
       " 'a',\n",
       " 's',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " 'e',\n",
       " 'r',\n",
       " '\\n',\n",
       " '1',\n",
       " '2',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'H',\n",
       " 'a',\n",
       " 'p',\n",
       " 'p',\n",
       " 'y',\n",
       " ' ',\n",
       " 's',\n",
       " 'o',\n",
       " ' ',\n",
       " 'f',\n",
       " 'a',\n",
       " 'r',\n",
       " ' ',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'l',\n",
       " 'a',\n",
       " 'c',\n",
       " 'e',\n",
       " 'm',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " '\\n',\n",
       " '1',\n",
       " '3',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'H',\n",
       " 'e',\n",
       " 'a',\n",
       " 'l',\n",
       " 't',\n",
       " 'h',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " 's',\n",
       " 'u',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'j',\n",
       " 'o',\n",
       " 'b',\n",
       " '.',\n",
       " ' ',\n",
       " 'N',\n",
       " 'o',\n",
       " 't',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'g',\n",
       " 'o',\n",
       " 'o',\n",
       " 'd',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 't',\n",
       " '.',\n",
       " ' ',\n",
       " 'M',\n",
       " 'a',\n",
       " 'n',\n",
       " 'u',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '\\n',\n",
       " '1',\n",
       " '4',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'E',\n",
       " 'r',\n",
       " 'r',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'r',\n",
       " 'a',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " 'n',\n",
       " 'n',\n",
       " 'a',\n",
       " 'c',\n",
       " 'e',\n",
       " 'p',\n",
       " 't',\n",
       " 'a',\n",
       " 'b',\n",
       " 'l',\n",
       " 'e',\n",
       " '\\n',\n",
       " '1',\n",
       " '5',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'M',\n",
       " 'a',\n",
       " 'n',\n",
       " 'y',\n",
       " ' ',\n",
       " 'u',\n",
       " 'n',\n",
       " 's',\n",
       " 'c',\n",
       " 'h',\n",
       " 'e',\n",
       " 'd',\n",
       " 'u',\n",
       " 'l',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 'b',\n",
       " 's',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'e',\n",
       " 's',\n",
       " '\\n',\n",
       " '1',\n",
       " '6',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'N',\n",
       " 'o',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'l',\n",
       " 'i',\n",
       " 'a',\n",
       " 'b',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'o',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 't',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 's',\n",
       " 'p',\n",
       " 'o',\n",
       " 'r',\n",
       " 't',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " '\\n',\n",
       " '1',\n",
       " '7',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'Y',\n",
       " 'o',\n",
       " 'u',\n",
       " 'r',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'c',\n",
       " 'e',\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "    \n",
    "comments = worker_data[\"Comments from the employer\"]\n",
    "comments = str(comments)\n",
    "    #comments = pd.Series(comments).str.cat(sep=' ')\n",
    "    \n",
    "for comment in comments:\n",
    "    if (comment == None) or (comment == 'NA') or (len(comment) < 4):\n",
    "        scores.append(comment)\n",
    "    else:\n",
    "        splitter = Splitter()\n",
    "        postagger = POSTagger()\n",
    "        dicttagger = DictionaryTagger(['dicts/positive.yml','dicts/negative.yml','dicts/inc.yml','dicts/dec.yml','dicts/inv.yml'])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
