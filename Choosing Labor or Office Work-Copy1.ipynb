{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the Ideal Job Type for an Applicant\n",
    "\n",
    "### Using clustering analysis to find the right fit\n",
    "\n",
    "In the previous project, an attempt was made to create a best-fit model to predict the future success of a worker brought on by a recruiter. In this project, we will see if we can predict the best type of work for a new recruit, to best guarantee success in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pull in and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read in the worker data\n",
    "\n",
    "xls_file = pd.ExcelFile(\"Origami_Data.xlsx\", encoding = 'utf-8')\n",
    "worker_data = xls_file.parse('Client Information')\n",
    "print \"worker data read successfully!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers in the office field: 101\n",
      "Number of workers in the manual labor field: 98\n"
     ]
    }
   ],
   "source": [
    "n_office = np.shape(worker_data[worker_data['OFFICE/MANUAL']=='OFFICE'])[0]\n",
    "n_manual = np.shape(worker_data[worker_data['OFFICE/MANUAL']=='MANUAL'])[0]\n",
    "\n",
    "print \"Number of workers in the office field: {}\".format(n_office)\n",
    "print \"Number of workers in the manual labor field: {}\".format(n_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have a good balance of both types of work, but we want to find just the workers who have been succesful in the past, so let's eliminate the ones who did not last 6 months.\n",
    "\n",
    "#### Clean up values\n",
    "First make sure to clean up non-consistent data in columns state and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make all state data shorthand and include gender only with M or F\n",
    "worker_data['State'] = map(lambda x: x.lower(), worker_data['State'])\n",
    "worker_data = worker_data.replace({'alabama':'al','florida':'fl','georgia':'ga','south carolina':'sc','louisiana':'la'}, regex=True)\n",
    "states = ['al','fl','ga','la','sc']\n",
    "worker_data = worker_data.loc[worker_data['State'].isin(states)]\n",
    "gender = ['M','F']\n",
    "worker_data = worker_data.loc[worker_data['Gender'].isin(gender)]\n",
    "\n",
    "# Remove NaN\n",
    "worker_data2 = worker_data.dropna(axis = 0, how = 'any', subset = ['Employed In Past 6 Months','Gender','Age','State','Education Level'])\n",
    "\n",
    "# Excluse the failures\n",
    "worker_data3 = worker_data2[worker_data2['Placement Successful'] == 'Y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract feature (X) and target (y) columns, and removing ID and Comments columns\n",
    "feature_cols = ['Employed In Past 6 Months','Gender','Age','State','Education Level','Placement Successful']\n",
    "target_col = ['OFFICE/MANUAL']\n",
    "\n",
    "\n",
    "x_all = worker_data3[feature_cols]\n",
    "y_all = worker_data3[target_col]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess feature columns\n",
    "\n",
    "It turns out there are a few non-numeric columns that need to be converted! One of them is simply `yes`/`no`, e.g. `'Employed In Past 6 Months'`. This can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `State` and `Education Level`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `AL`, `GA`, `FL`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are called _dummy variables_, and so we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to create these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (15):-\n",
      "[u'Employed In Past 6 Months', u'Gender_F', u'Gender_M', u'Age', u'State_al', u'State_fl', u'State_ga', u'State_la', u'State_sc', u'Education Level_College Dropout ', u'Education Level_College Graduate', u'Education Level_High School Drop Out', u'Education Level_High School Graduate', u'Education Level_Technical School', u'Placement Successful_Y']\n"
     ]
    }
   ],
   "source": [
    "# Convert the target feature Y/N -Placement Successful- to 1/0\n",
    "y_all = y_all.replace({'Y':1, 'N':0})\n",
    "\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['Yes', 'No'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. 'state' => 'state_AL', 'state_GA'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "x_all = preprocess_features(x_all)\n",
    "\n",
    "print \"Processed feature columns ({}):-\\n{}\".format(len(x_all.columns), list(x_all.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Split data into training and test sets\n",
    "\n",
    "So far, we have converted all _categorical_ features into numeric. In this next step, we split the data (both features and targets) into training and test sets. We will use this data to create a model that will be tested against the final data from Origami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 94 samples\n",
      "Test set: 24 samples\n"
     ]
    }
   ],
   "source": [
    "import sklearn.cross_validation as cv\n",
    "\n",
    "x_all_train, x_all_test, y_all_train, y_all_test = cv.train_test_split(x_all, y_all, test_size=.2, random_state=12)\n",
    "\n",
    "print \"Training set: {} samples\".format(x_all_train.shape[0])\n",
    "print \"Test set: {} samples\".format(x_all_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_all_train = y_all_train['OFFICE/MANUAL'].values.ravel()\n",
    "y_all_test = y_all_test['OFFICE/MANUAL'].values.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training and Evaluating Models\n",
    "\n",
    "#### Running multiple trials and finding the mean accuracy of classifier\n",
    "\n",
    "As the dataset is relatively small and the results can vary from run to run. We will average 50 trials to find the most likely probability to our future predictions, assuming we do not overfit too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score for this classifier over 50 trials is 71.39%\n"
     ]
    }
   ],
   "source": [
    "# Import the random forest package\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "all_scores = []\n",
    "\n",
    "for x in range(50):\n",
    "    x_all_train, x_all_test, y_all_train, y_all_test = cv.train_test_split(x_all, y_all, test_size=.3)\n",
    "    clf = RandomForestClassifier(n_estimators = 30)\n",
    "    clf = clf.fit(x_all_train,y_all_train.values.ravel())\n",
    "    all_scores.append(clf.score(x_all_test, y_all_test.values))\n",
    "\n",
    "print \"The average score for this classifier over 50 trials is {:.2%}\".format(np.mean(all_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
